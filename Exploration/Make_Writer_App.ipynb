{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Make_Writer_App.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOYhKe9TVDhBCRAEomLo9vg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wgJ48rDEw0W4","executionInfo":{"status":"ok","timestamp":1626932453540,"user_tz":-540,"elapsed":27063,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"05baf531-1137-4870-9d12-c8398c905e7b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F6cHV4jtxD89"},"source":["# 인공지능 각본가/작사가 만들기"]},{"cell_type":"markdown","metadata":{"id":"-ENRURa-n4kI"},"source":["# 0. 인공지능의 작문"]},{"cell_type":"markdown","metadata":{"id":"ef6Ocf7Bn99G"},"source":["#### 현존하는 최고의 인공지능 작문가 GPT-2(https://talktotransformer.com/)  \n","- GPT-2 이전에도 작문을 할 수 있는 딥러닝 모델은 존재했지만, 생성한 문장 길이가 일정 길이 이상이 되면, 주제의 일관성이 흐트러지고 어색해짐\n","- 하지만 GPT-2는 신문기사 1편 정도의 길이의 글을 작문하면서, **주제나 논리의 일관성을 어느정도 유지**\n","- 19년 2월, GPT-2를 발표한 OpenAI에서 **문장 생성 모델의 오남용이 가져올 위험 때문에 해당 모델을 비공개**하기로 하면서 [기사](https://decenter.kr/NewsView/1VFGQMBBXZ/GZ02)들이 한동안 세간의 이슈가 되었음\n","- 그로부터 1년여 후 2020년 5월에 OpenAI에서는 다시 GPT-2를 이전보다 훨씬 큰 규모로 발전시킨 **GPT-3**를 발표. GPT-3가 만들어낸 텍스트는 그저 논리적 일관성을 유지하는 수준을 넘어서서 **사람이 쓴 것과 구분이 안될 정도의 자연스러움**을 보여줌\n","\n","#### 그렇다면 왜 작문하는 인공지능이 이토록 충격을 주는 것일까?\n","- 인공지능이 사람의 언어에서 생각과 의도와 감정을 읽어 내고, 그 의미를 이해하며, 적절한 말을 만들어내서 인간의 질문에 대답할 수 있다면 그야말로 지능을 가진 기계라는 특이점에 이르렀다고 할 수 있기 때문\n","\n","\n","\n","\n","#### 🔥 **인공지능이 문장을 이해하는 방식과 작문을 가르치는 법을 공부하고, 구현해 보자 !** 🔥\n"]},{"cell_type":"markdown","metadata":{"id":"33OO7VdM8eW9"},"source":["# 1. 인공지능 각본가 만들기"]},{"cell_type":"markdown","metadata":{"id":"jns5e8eI8XYV"},"source":["## 1-1. 데이터 전처리\n","① 정규표현식을 이용한 corpus 생성  \n","② `tf.keras.preprocessing.text.Tokenizer`를 이용해 corpus를 텐서로 변환  \n","③ `tf.data.Dataset.from_tensor_slices()`를 이용해 corpus 텐서를 `tf.data Dataset`객체로 변환"]},{"cell_type":"markdown","metadata":{"id":"k4o0-Ec89LLM"},"source":["### 1-1-1. 데이터 확인"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ZiTDPcX4-N7","executionInfo":{"status":"ok","timestamp":1626932462426,"user_tz":-540,"elapsed":3351,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"16b6ef64-a64b-4465-86b5-e5039b34091c"},"source":["import re \n","import numpy as np\n","import tensorflow as tf\n","\n","# 파일을 읽기모드로 열고\n","# 라인 단위로 끊어서 list 형태로 읽어옵니다.\n","file_path = '/content/drive/MyDrive/Colab Notebooks/Data-Science/data/lyricist/shakespeare.txt'\n","with open(file_path, \"r\") as f:\n","    raw_corpus = f.read().splitlines()\n","\n","# 앞에서부터 10라인만 화면에 출력해 볼까요?\n","print(raw_corpus[:9])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["['First Citizen:', 'Before we proceed any further, hear me speak.', '', 'All:', 'Speak, speak.', '', 'First Citizen:', 'You are all resolved rather to die than to famish?', '']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1JLwYLBp8zhO"},"source":["문장(대사) 만을 원하므로 화자 이름이나 공백뿐인 정보는 필요없음\n","- 화자가 표기된 문장 (`~:`)\n","- 공백인 문장(`len=0`) 제거"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gy8AQsJI8tnb","executionInfo":{"status":"ok","timestamp":1626932464127,"user_tz":-540,"elapsed":470,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"88d7f623-c515-44ef-b918-ccbd12095033"},"source":["#cnt=0\n","for idx, sentence in enumerate(raw_corpus):\n","    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n","    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n","    \n","    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n","    #cnt+=1    \n","    print(sentence)\n","#print(cnt) # 총 문장수 : 24015"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Before we proceed any further, hear me speak.\n","Speak, speak.\n","You are all resolved rather to die than to famish?\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"70eedFMw9Pqb"},"source":["### 1-2-1. Tokenize"]},{"cell_type":"markdown","metadata":{"id":"SFiOC3GN9Vzt"},"source":["텍스트 생성 모델의 경우에도 단어 사전을 만듦 => **문장을 일정한 기준으로 쪼개기**  \n","\n","**공백을 기준으로 토큰화하자!**  \n","주의사항\n","1. Hi, my name is John. *(\"Hi,\" \"my\", ..., \"john.\" 으로 분리됨) - 문장부호  \n","  -> 문장 부호 양쪽에 공백 추가\n","\n","2. First, open the first chapter. *(First와 first를 다른 단어로 인식) - 대소문자  \n","  -> 모든 문자들을 소문자로 변환\n","3. He is a ten-year-old boy. *(ten-year-old를 한 단어로 인식) - 특수문자  \n","  -> 특수문자 모두 제거"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPPt2JyG9D74","executionInfo":{"status":"ok","timestamp":1626932467491,"user_tz":-540,"elapsed":311,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"d8a8cd48-52a5-4921-b30a-0b2d73a28303"},"source":["# 입력된 문장을\n","#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n","#     2. 특수문자 양쪽에 공백을 넣고\n","#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n","#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n","#     5. 다시 양쪽 공백을 지웁니다\n","#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n","# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n","def preprocess_sentence(sentence):\n","    sentence = sentence.lower().strip() # 1\n","    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n","    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n","    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n","    sentence = sentence.strip() # 5\n","    sentence = '<start> ' + sentence + ' <end>' # 6\n","    return sentence\n","\n","# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n","print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["<start> this is sample sentence . <end>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vb8jyvAbEViE"},"source":["**소스 문장(Source Sentence)** : 모델의 입력이 되는 문장. `X_train`  \n","**타겟 문장(Target Sentence)** : 정답 역할을 하게 될 모델의 출력 문장. `y_train`"]},{"cell_type":"markdown","metadata":{"id":"Yz5-GdsUE0uH"},"source":["### 1-2-2. 정제 데이터 구축"]},{"cell_type":"markdown","metadata":{"id":"paOEboJ-E-2b"},"source":["정제 함수를 통해 만든 데이터셋에서 토큰화를 진행한 후 끝 단어 <end>를 없애면 **소스 문장**, 첫 단어 <start>를 없애면 타겟 문장"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L_lhCb2qEK29","executionInfo":{"status":"ok","timestamp":1626932475094,"user_tz":-540,"elapsed":5316,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"6333c9ca-8853-4594-a76c-93c8571136c8"},"source":["# 여기에 정제된 문장을 모을겁니다\n","corpus = []\n","\n","for sentence in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","    \n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","    corpus.append(preprocessed_sentence)\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus[:10]"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> before we proceed any further , hear me speak . <end>',\n"," '<start> speak , speak . <end>',\n"," '<start> you are all resolved rather to die than to famish ? <end>',\n"," '<start> resolved . resolved . <end>',\n"," '<start> first , you know caius marcius is chief enemy to the people . <end>',\n"," '<start> we know t , we know t . <end>',\n"," '<start> let us kill him , and we ll have corn at our own price . <end>',\n"," '<start> is t a verdict ? <end>',\n"," '<start> no more talking on t let it be done away , away ! <end>',\n"," '<start> one word , good citizens . <end>']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"avxH9kliF2d_"},"source":["### 1-3-1. Vectorize"]},{"cell_type":"markdown","metadata":{"id":"9rkhy-9VF8zI"},"source":["**데이터 사전을 활용해 언어 데이터를 숫자로 변환**  \n","인공지능 (숫자) <-> 사람 (언어)  \n"]},{"cell_type":"markdown","metadata":{"id":"ZFRu2MYXGXT5"},"source":["`tf.keras.preprocessing.text.Tokenizer`를 활용해 vectorize 하자\n","- 정제된 데이터를 토큰화\n","- 단어 사전(vocabulary 또는 dictionary라고 칭함) 만듦\n","- 데이터를 숫자로 변환 (변환된 데이터 : `tensor`)\n","  - [Tensor란 무엇인가?](https://rekt77.tistory.com/102)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3SJQYj2tFLG5","executionInfo":{"status":"ok","timestamp":1626932479758,"user_tz":-540,"elapsed":761,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"77c12c9a-6786-4546-93fa-7876d21d0f3e"},"source":["# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n","# 더 잘 알기 위해 아래 문서들을 참고하면 좋습니다\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n","def tokenize(corpus):\n","    # 7000단어를 기억할 수 있는 tokenizer를 만들겁니다\n","    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n","    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=7000, \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    tokenizer.fit_on_texts(corpus)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    tensor = tokenizer.texts_to_sequences(corpus)   \n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n","    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n","    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n","    \n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[[   2  143   40 ...    0    0    0]\n"," [   2  110    4 ...    0    0    0]\n"," [   2   11   50 ...    0    0    0]\n"," ...\n"," [   2  149 4553 ...    0    0    0]\n"," [   2   34   71 ...    0    0    0]\n"," [   2  945   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f411b435410>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nsAbYO1-G-sI"},"source":["텐서 데이터 : tokenizer에 구축된 단어 사전의 인덱스 (type int)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQmHnKYhG6ho","executionInfo":{"status":"ok","timestamp":1626932482027,"user_tz":-540,"elapsed":325,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"29a002c1-2932-481a-9f8d-cce720609dca"},"source":["# 단어 사전을 확인해보자\n","for idx in tokenizer.index_word:\n","    print(idx, \":\", tokenizer.index_word[idx])\n","\n","    if idx >= 10: break\n","print(143,\":\", tokenizer.index_word[143])    "],"execution_count":8,"outputs":[{"output_type":"stream","text":["1 : <unk>\n","2 : <start>\n","3 : <end>\n","4 : ,\n","5 : .\n","6 : the\n","7 : and\n","8 : i\n","9 : to\n","10 : of\n","143 : before\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bUeZZfojHgAP"},"source":["### 1-3-2. vectorize 된 tensor를 소스와 타겟으로 분리"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dr80bNOJHWd4","executionInfo":{"status":"ok","timestamp":1626932484896,"user_tz":-540,"elapsed":383,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"6817fd2f-c962-47e3-8f06-150f19d7ea7e"},"source":["# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n","# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n","src_input = tensor[:, :-1]  \n","# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n","tgt_input = tensor[:, 1:]    \n","\n","print(src_input[0]) # <start>~~ .\n","print(tgt_input[0]) # ~~~ <end>(또는 <pad>)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[  2 143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0\n","   0   0]\n","[143  40 933 140 591   4 124  24 110   5   3   0   0   0   0   0   0   0\n","   0   0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CSaNxGTBHuZC"},"source":["텐서 출력부에서 행 뒤쪽에 0이 많이 나온 부분은 정해진 입력 시퀀스 길이보다 문장이 짧을 경우 0으로 패딩(padding)을 채워 넣은 것입니다. 사전에는 없지만 0은 바로 패딩 문자 <pad>가 될 것입니다."]},{"cell_type":"markdown","metadata":{"id":"jDdVcCfzJFup"},"source":["### 1-3-3. 데이터셋 객체 생성"]},{"cell_type":"markdown","metadata":{"id":"u1zULrzfJMYW"},"source":["`Rock_Scissor_Paper`, `MNIST handwritten digit classification` 등  \n"," : model.fit(x_train, y_train, ...) 형태로 Numpy Array 데이터셋을 생성하고, model에 제공해 train  \n","\n","**텐서플로우를 활용할 경우** 텐서로 생성된 데이터를 이용해 `tf.data.Dataset`객체를 생성함  \n","`tf.data.Dataset` 객체는 텐서플로우에서 사용할 경우 데이터 입력 파이프라인을 통한 속도 개선 및 각종 편의 기능을 제공함"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wqA01NLJHnM","executionInfo":{"status":"ok","timestamp":1626932493310,"user_tz":-540,"elapsed":6224,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"8639c4f3-cbeb-4b6c-aeeb-9e6b276846db"},"source":["# tf.data.Dataset.from_tensor_slices() 메소드를 이용해 tf.data.Dataset객체를 생성\n","\n","BUFFER_SIZE = len(src_input) # 총 문장 수 : 24015\n","BATCH_SIZE = 256\n","steps_per_epoch = len(src_input) // BATCH_SIZE\n","\n"," # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n","VOCAB_SIZE = tokenizer.num_words + 1   \n","\n","# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n","# 데이터셋에 대해서는 아래 문서를 참고하세요\n","# 자세히 알아둘수록 도움이 많이 되는 중요한 문서입니다\n","# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n","dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n","dataset = dataset.shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n","dataset"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset shapes: ((256, 20), (256, 20)), types: (tf.int32, tf.int32)>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"LDF0zpFTNRj4"},"source":["## 1-2. 인공지능(model) 학습시키기"]},{"cell_type":"markdown","metadata":{"id":"-_awPkhGNrUl"},"source":["`tf.keras.Model`을 Subclassing하는 방식으로 model 구현  \n","\n","**구현할 model** =  1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성  \n",".  \n","![image](https://user-images.githubusercontent.com/45934191/126585310-e8aa962e-4aa0-4d1e-99b9-090ac4dbc024.png)\n"]},{"cell_type":"code","metadata":{"id":"lt_begq1Nq89","executionInfo":{"status":"ok","timestamp":1626932498217,"user_tz":-540,"elapsed":335,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}}},"source":["class TextGenerator(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_size, hidden_size):\n","        super().__init__()\n","        \n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n","        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n","        self.linear = tf.keras.layers.Dense(vocab_size)\n","        \n","    def call(self, x):\n","        out = self.embedding(x)\n","        out = self.rnn_1(out)\n","        out = self.rnn_2(out)\n","        out = self.linear(out)\n","        \n","        return out\n","    \n","embedding_size = 256 # 워드 벡터의 차원수\n","hidden_size = 1024\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VCGiPBNwOT7A"},"source":["\n","`Embedding 레이어` : (입력 텐서의) **인덱스** 값 -> 해당 인덱스 번째의 **워드 벡터**로 변환\n","  - 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현 (representation)으로 사용됨\n","\n","`embedding_size `  \n","- 워드 벡터의 차원수. 즉 단어가 추상적으로 표현되는 크기 !!\n","- 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만, 그만큼 충분한 데이터가 주어지지 않으면 오히려 혼란을 야기함\n","> ex. `embedding_size` = 2  \n","차갑다: [0.0, 1.0]  \n","뜨겁다: [1.0, 0.0]  \n","미지근하다: [0.5, 0.5]\n","\n","`hidden_size `\n","- `LSTM 레이어`의 `hidden state` 의 차원수\n","- 모델에 얼마나 많은 일꾼을 둘 것인가?\n","- 일꾼들은 모두 같은 데이터를 보고 각자의 생각을 가지는데, 역시 충분한 데이터가 주어지면 올바른 결정을 내리겠지만 그렇지 않으면 배가 산으로 감 "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WdIPKKxuPJ85","executionInfo":{"status":"ok","timestamp":1626932521182,"user_tz":-540,"elapsed":8418,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"22e26f03-b2eb-48a9-c6cb-f7dca6b2fcb0"},"source":["# model build 전( model.compile()을 호출x, model의 입력 텐서 지정x) 에 데이터 좀 태워보기 \n","# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n","# 지금은 동작 원리에 너무 빠져들지 마세요~\n","for src_sample, tgt_sample in dataset.take(1): break\n","\n","# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n","model(src_sample)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(256, 20, 7001), dtype=float32, numpy=\n","array([[[ 5.49086486e-04,  1.66521437e-04,  1.00034005e-04, ...,\n","         -3.54421034e-04,  1.98820562e-04, -8.35924657e-05],\n","        [ 7.75427208e-04,  5.06622659e-04, -4.77696040e-06, ...,\n","         -5.44135924e-04,  3.73839139e-04,  3.48993563e-05],\n","        [ 1.04616862e-03,  1.08062127e-03,  1.63546516e-04, ...,\n","         -6.86582702e-04,  4.72771382e-04, -2.25202530e-04],\n","        ...,\n","        [-5.23498573e-04,  3.32579832e-03, -3.81532795e-04, ...,\n","         -2.90569663e-03, -4.98008390e-04, -2.25709798e-03],\n","        [-6.60168065e-04,  3.53993522e-03, -9.09162685e-04, ...,\n","         -3.32693639e-03, -5.01539616e-04, -2.56467727e-03],\n","        [-7.90796010e-04,  3.67940776e-03, -1.43815309e-03, ...,\n","         -3.73980496e-03, -4.84170479e-04, -2.84078182e-03]],\n","\n","       [[ 5.49086486e-04,  1.66521437e-04,  1.00034005e-04, ...,\n","         -3.54421034e-04,  1.98820562e-04, -8.35924657e-05],\n","        [ 9.61516227e-04,  3.30343610e-04,  1.14572009e-04, ...,\n","         -5.27285971e-04,  4.27006889e-04, -3.64487903e-04],\n","        [ 8.03303381e-04,  3.30704177e-04,  1.25434046e-04, ...,\n","         -6.61333208e-04,  2.04581767e-04, -8.11884704e-04],\n","        ...,\n","        [-5.61298453e-04,  3.64334183e-03, -1.29560416e-03, ...,\n","         -2.66165263e-03, -1.33760436e-03, -2.28238874e-03],\n","        [-6.99452998e-04,  3.79877444e-03, -1.68828294e-03, ...,\n","         -3.27439490e-03, -1.25688454e-03, -2.53685517e-03],\n","        [-8.38598236e-04,  3.88290570e-03, -2.08940776e-03, ...,\n","         -3.81495152e-03, -1.15488982e-03, -2.76506925e-03]],\n","\n","       [[ 5.49086486e-04,  1.66521437e-04,  1.00034005e-04, ...,\n","         -3.54421034e-04,  1.98820562e-04, -8.35924657e-05],\n","        [ 8.01866874e-04, -9.22081836e-06, -5.58493703e-05, ...,\n","         -7.16173497e-04,  4.75707675e-05,  1.35122958e-04],\n","        [ 8.99231760e-04,  3.79004159e-05,  8.21707581e-05, ...,\n","         -5.86400041e-04, -1.99842718e-04, -2.85849586e-04],\n","        ...,\n","        [-5.14059211e-04,  3.48997233e-03, -5.19550696e-04, ...,\n","         -2.89795687e-03, -1.62076540e-04, -2.93864915e-03],\n","        [-7.16914656e-04,  3.71833355e-03, -1.07551937e-03, ...,\n","         -3.33764777e-03, -2.09011880e-04, -3.10614984e-03],\n","        [-8.97565682e-04,  3.86630488e-03, -1.60630234e-03, ...,\n","         -3.76011152e-03, -2.36566470e-04, -3.25171091e-03]],\n","\n","       ...,\n","\n","       [[ 5.49086486e-04,  1.66521437e-04,  1.00034005e-04, ...,\n","         -3.54421034e-04,  1.98820562e-04, -8.35924657e-05],\n","        [ 8.80192441e-04,  7.02688485e-05,  1.21352226e-04, ...,\n","         -7.32197426e-04,  1.06955013e-04, -3.86114581e-04],\n","        [ 1.09374756e-03,  2.25409691e-04,  2.05743781e-05, ...,\n","         -1.08910992e-03, -4.90798615e-04, -4.83736017e-04],\n","        ...,\n","        [-3.94589588e-04,  2.49583577e-03, -1.36886828e-03, ...,\n","         -2.33932585e-03, -9.60368779e-04, -2.76648463e-03],\n","        [-5.65544760e-04,  2.92092771e-03, -1.78869115e-03, ...,\n","         -2.87514459e-03, -1.00506260e-03, -2.96805473e-03],\n","        [-7.32620072e-04,  3.23651149e-03, -2.21136864e-03, ...,\n","         -3.38064181e-03, -9.99875250e-04, -3.14646517e-03]],\n","\n","       [[ 5.49086486e-04,  1.66521437e-04,  1.00034005e-04, ...,\n","         -3.54421034e-04,  1.98820562e-04, -8.35924657e-05],\n","        [ 2.79518368e-04,  2.13020583e-04,  2.90337717e-04, ...,\n","         -4.78625676e-04,  2.15941502e-04,  1.28080108e-04],\n","        [-1.34651811e-04,  9.11707248e-05,  5.32209116e-04, ...,\n","         -3.65967077e-04,  3.68773530e-04,  4.67820028e-05],\n","        ...,\n","        [-9.54888645e-04,  3.46829952e-03, -2.86966632e-03, ...,\n","         -4.70368471e-03, -8.87989067e-04, -3.21730040e-03],\n","        [-1.07875175e-03,  3.52334580e-03, -3.24984593e-03, ...,\n","         -5.00110770e-03, -8.14669707e-04, -3.35172936e-03],\n","        [-1.19691424e-03,  3.54486890e-03, -3.59886023e-03, ...,\n","         -5.24867652e-03, -7.55588291e-04, -3.46787367e-03]],\n","\n","       [[ 5.49086486e-04,  1.66521437e-04,  1.00034005e-04, ...,\n","         -3.54421034e-04,  1.98820562e-04, -8.35924657e-05],\n","        [ 1.01026869e-03,  3.00112326e-04,  3.24810913e-04, ...,\n","         -6.57184981e-04,  1.13442286e-04, -3.02958884e-04],\n","        [ 1.38408202e-03,  2.86257389e-04,  2.21019887e-04, ...,\n","         -1.15964888e-03, -8.32431833e-05, -3.84206360e-04],\n","        ...,\n","        [ 7.12029345e-04,  2.57344847e-03, -2.00995267e-03, ...,\n","         -3.06903059e-03, -1.87444291e-03, -2.42499704e-03],\n","        [ 3.83559673e-04,  2.84227962e-03, -2.39260751e-03, ...,\n","         -3.58695979e-03, -1.76044425e-03, -2.73200194e-03],\n","        [ 7.85538941e-05,  3.04050441e-03, -2.76254676e-03, ...,\n","         -4.04792279e-03, -1.62959762e-03, -2.99336365e-03]]],\n","      dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"iy4J23-EhZZj"},"source":["`shape=(batch_size, return_sequence 길이, Dense 레이어 출력 차원수)`  \n","- `batch_size` : 256. `dataset.take(1)`를 통해서 1개의 배치를 가져옴(256개의 문장 데이터)\n","- `Dense레이어 출력차원수` : 7001. 7001개의 단어 중 어느 단어의 확률이 가장 높을지 모델링\n","- `return_sequence 길이` \n","  - `tf.keras.layers.LSTM(hidden_size, return_sequences=True)`로 호출한 LSTM 레이어에서 `return_sequences=True` 라고 지정한 부분 \n","  - LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력한다는 의미\n","  - `return_sequences=False` 였다면 LSTM 레이어는 1개의 벡터만 출력\n","  > Q. 모델은 입력 데이터의 시퀀스 길이가 얼마인지 모르고, 모델을 만들면서 알려준 적도 없음. 20은 언제 알게된 것일까?  \n","  A. 데이터를 입력받으면서 비로소 알게 된 것. 데이터셋의 max_len이 20으로 맞춰져 있었음\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"tVXyV2J7KL8u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626932525220,"user_tz":-540,"elapsed":318,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"32bc1a66-0eea-4041-b436-0a8097ecda41"},"source":["model.summary()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Model: \"text_generator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        multiple                  1792256   \n","_________________________________________________________________\n","lstm (LSTM)                  multiple                  5246976   \n","_________________________________________________________________\n","lstm_1 (LSTM)                multiple                  8392704   \n","_________________________________________________________________\n","dense (Dense)                multiple                  7176025   \n","=================================================================\n","Total params: 22,607,961\n","Trainable params: 22,607,961\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MKUsXuRdjAUy"},"source":["`model.summary()`를 호출해도, `Output Shape`를 정확하게 알려주지 않음  \n","=> 모델은 입력 시퀀스의 길이를 모르기 때문에 Output Shape를 특정할 수 없음\n","\n","하지만 모델의 파라미터 사이즈는 대략 `22million` 정도로 측정됨.  \n","(cf. **GPT-2**의 파라미터 사이즈  1.5billion, **GPT-3**의 파라미터 사이즈=GPT-2* 100)"]},{"cell_type":"markdown","metadata":{"id":"fhbm3C8Yj_C6"},"source":["### **Train 🔥🔥**"]},{"cell_type":"code","metadata":{"id":"Mofo1FxbO9Xf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626933119953,"user_tz":-540,"elapsed":591810,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"97268d11-4888-403b-98e9-d6f5db100e6b"},"source":["# optimizer와 loss등은 차차 배웁니다\n","# 혹시 미리 알고 싶다면 아래 문서를 참고하세요\n","# https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n","# https://www.tensorflow.org/api_docs/python/tf/keras/losses\n","# 양이 상당히 많은 편이니 지금 보는 것은 추천하지 않습니다\n","optimizer = tf.keras.optimizers.Adam()\n","tf.test.is_gpu_available()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True,\n","    reduction='none'\n",")\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","model.fit(dataset, epochs=30)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-14-550b28a5250d>:7: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.config.list_physical_devices('GPU')` instead.\n","Epoch 1/30\n","93/93 [==============================] - 20s 186ms/step - loss: 3.4717\n","Epoch 2/30\n","93/93 [==============================] - 18s 194ms/step - loss: 2.8037\n","Epoch 3/30\n","93/93 [==============================] - 19s 205ms/step - loss: 2.7003\n","Epoch 4/30\n","93/93 [==============================] - 20s 212ms/step - loss: 2.5981\n","Epoch 5/30\n","93/93 [==============================] - 19s 205ms/step - loss: 2.5317\n","Epoch 6/30\n","93/93 [==============================] - 19s 203ms/step - loss: 2.4794\n","Epoch 7/30\n","93/93 [==============================] - 19s 203ms/step - loss: 2.4220\n","Epoch 8/30\n","93/93 [==============================] - 19s 206ms/step - loss: 2.3699\n","Epoch 9/30\n","93/93 [==============================] - 19s 207ms/step - loss: 2.3198\n","Epoch 10/30\n","93/93 [==============================] - 19s 205ms/step - loss: 2.2722\n","Epoch 11/30\n","93/93 [==============================] - 19s 203ms/step - loss: 2.2256\n","Epoch 12/30\n","93/93 [==============================] - 19s 205ms/step - loss: 2.1807\n","Epoch 13/30\n","93/93 [==============================] - 19s 206ms/step - loss: 2.1323\n","Epoch 14/30\n","93/93 [==============================] - 19s 205ms/step - loss: 2.0856\n","Epoch 15/30\n","93/93 [==============================] - 19s 205ms/step - loss: 2.0402\n","Epoch 16/30\n","93/93 [==============================] - 19s 204ms/step - loss: 1.9941\n","Epoch 17/30\n","93/93 [==============================] - 19s 204ms/step - loss: 1.9466\n","Epoch 18/30\n","93/93 [==============================] - 19s 204ms/step - loss: 1.8992\n","Epoch 19/30\n","93/93 [==============================] - 19s 204ms/step - loss: 1.8505\n","Epoch 20/30\n","93/93 [==============================] - 19s 205ms/step - loss: 1.8031\n","Epoch 21/30\n","93/93 [==============================] - 19s 206ms/step - loss: 1.7557\n","Epoch 22/30\n","93/93 [==============================] - 19s 205ms/step - loss: 1.7083\n","Epoch 23/30\n","93/93 [==============================] - 19s 205ms/step - loss: 1.6612\n","Epoch 24/30\n","93/93 [==============================] - 19s 204ms/step - loss: 1.6126\n","Epoch 25/30\n","93/93 [==============================] - 19s 205ms/step - loss: 1.5656\n","Epoch 26/30\n","93/93 [==============================] - 19s 204ms/step - loss: 1.5193\n","Epoch 27/30\n","93/93 [==============================] - 19s 206ms/step - loss: 1.4710\n","Epoch 28/30\n","93/93 [==============================] - 19s 206ms/step - loss: 1.4242\n","Epoch 29/30\n","93/93 [==============================] - 19s 205ms/step - loss: 1.3747\n","Epoch 30/30\n","93/93 [==============================] - 19s 204ms/step - loss: 1.3258\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f411b0ac710>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"U7KKqW-MksX1"},"source":["Loss : 모델이 오답을 만들고 있는 정도  \n","(Loss가 1일 때 99%를 맞추고 있다는 의미는 아님).  \n","오답률이 감소하고 있으니 **학습이 잘 진행되고 있다** 고 해석할 수 있다!"]},{"cell_type":"markdown","metadata":{"id":"_H1c0uG6kW3p"},"source":[">학습엔 15분 정도 소요됨(GPU 환경 기준)  \n","혹시라도 지나치게 많은 시간이 소요된다면 `tf.test.is_gpu_available()` 를 실행해 텐서플로우가 GPU를 잘 사용하고 있는지 확인하자  \n","**또는 런타임 > 런타임 유형 변경 > 하드웨어 가속기에서 gpu를 가속시키자**\n"]},{"cell_type":"markdown","metadata":{"id":"fo8mThp-woKc"},"source":["## 1-3. 평가하기"]},{"cell_type":"markdown","metadata":{"id":"s4ilaVxZxVAq"},"source":["모델이 작문을 잘하는지 컴퓨터 알고리즘이 평가하는 것은 무리가 있다.  \n","**작문을 시켜보고 직접 평가하자 !**"]},{"cell_type":"code","metadata":{"id":"J3G5Wh-vO_7E","executionInfo":{"status":"ok","timestamp":1626933212326,"user_tz":-540,"elapsed":320,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}}},"source":["# init sentense -> 작문\n","def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n","    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n","    test_input = tokenizer.texts_to_sequences([init_sentence])\n","    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n","    end_token = tokenizer.word_index[\"<end>\"]\n","\n","    # test dataset 타겟 문장, 소스 문장이 없음\n","    # 단어 하나씩 예측해 문장을 만듭니다\n","    #    1. 입력받은 문장의 텐서를 입력합니다\n","    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n","    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n","    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n","    while True:\n","        # 1\n","        predict = model(test_tensor) \n","        # 2\n","        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n","        # 3 \n","        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n","        # 4\n","        if predict_word.numpy()[0] == end_token: break\n","        if test_tensor.shape[1] >= max_len: break\n","\n","    generated = \"\"\n","    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n","    for word_index in test_tensor[0].numpy():\n","        generated += tokenizer.index_word[word_index] + \" \"\n","\n","    return generated"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mFJrOg4LyE-T"},"source":[">while의 첫 번째 루프에서 test_tensor에 <start> 하나만 들어가고, 모델이 출력으로 7001개의 단어 중 A를 골랐다고 가정.  \n","while의 두 번째 루프에서 test_tensor에는 <start> A가 들어가고, 모델이 그다음 B를 골랐다고 가정  \n","while의 세 번째 루프에서 test_tensor에는 <start> A B가 들어가고, …\n"]},{"cell_type":"code","metadata":{"id":"SKZ2MW24O_5V","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1626933573064,"user_tz":-540,"elapsed":358,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"9f356dde-213c-41c6-e634-087afa4a904a"},"source":["# 문장 생성 함수 실행\n","generate_text(model, tokenizer, init_sentence=\"<start> baby\")"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> baby in a twofold vigour lift me up <end> '"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"EYVvYG3w7hvN"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"7VeJYrNwzrW-"},"source":["# 2. 인공지능 작사가 만들기"]},{"cell_type":"markdown","metadata":{"id":"_LjZPDoa0tg1"},"source":["[Song Lyrics](https://www.kaggle.com/paultimothymooney/poetry/data) dataset 사용"]},{"cell_type":"markdown","metadata":{"id":"uvhA9Aqlz6Ug"},"source":["## 2-1. 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"HCaVHm0E8GEp"},"source":["### 데이터 읽어오기"]},{"cell_type":"code","metadata":{"id":"yyv9RkykO_3r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626944150582,"user_tz":-540,"elapsed":399,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"85fdbe77-09f7-406e-c382-91d3e94ed62a"},"source":["# 데이터 읽어오기\n","import glob\n","\n","txt_file_path = '/content/drive/MyDrive/Colab Notebooks/Data-Science/data/lyricist/lyrics/*'\n","\n","txt_list = glob.glob(txt_file_path)\n","\n","raw_corpus = []\n","\n","# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n","for txt_file in txt_list:\n","    with open(txt_file, \"r\") as f:\n","        raw = f.read().splitlines()\n","        raw_corpus.extend(raw)\n","\n","print(\"데이터 크기:\", len(raw_corpus))\n","print(\"Examples:\\n\", raw_corpus[:3])"],"execution_count":98,"outputs":[{"output_type":"stream","text":["데이터 크기: 187088\n","Examples:\n"," ['Looking for some education', 'Made my way into the night', 'All that bullshit conversation']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ufJWsM3q8Kf7"},"source":["### 전처리\n","지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로, (너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 ?)  \n","문장을 토큰화 했을 때 **토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외**하자\n","> 토큰의 개수 = 소스, 타켓 데이터 각각 `<end>`, `<start>`를 제외한 개수"]},{"cell_type":"code","metadata":{"id":"B6DhVpxCO_1G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626944155807,"user_tz":-540,"elapsed":2232,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"08bcd42d-6a92-4774-ea42-ba0efe2a79f0"},"source":["# 여기에 정제된 문장을 모을겁니다\n","corpus = []\n","\n","# Tokenize\n","# 정제 데이터 구축\n","for sentence in raw_corpus:\n","    # 우리가 원하지 않는 문장은 건너뜁니다\n","    if len(sentence) == 0: continue\n","    if sentence[-1] == \":\": continue\n","    # if len(sentence.split()) >= 14: continue\n","\n","    # 정제를 하고 담아주세요\n","    preprocessed_sentence = preprocess_sentence(sentence)\n","\n","    # sentence = can't(1) / preprocessed_sentense = can t(2)\n","    if len(preprocessed_sentence.split()) >= 16: continue # start, end 제외\n","\n","    corpus.append(preprocessed_sentence)\n","        \n","# 정제된 결과를 10개만 확인해보죠\n","corpus[:10]\n"],"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<start> looking for some education <end>',\n"," '<start> made my way into the night <end>',\n"," '<start> all that bullshit conversation <end>',\n"," '<start> i don t even wanna waste your time <end>',\n"," '<start> let s just say that maybe <end>',\n"," '<start> you could help me ease my mind <end>',\n"," '<start> if that s love in your eyes <end>',\n"," '<start> it s more than enough <end>',\n"," '<start> had some bad love <end>',\n"," '<start> ooh , ooh looking for some affirmation <end>']"]},"metadata":{"tags":[]},"execution_count":99}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Swq7fE6ZG479","executionInfo":{"status":"ok","timestamp":1626944161008,"user_tz":-540,"elapsed":4019,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"d765979d-1624-470c-8ae1-a7370f7d1808"},"source":["# Vectorize (언어 데이터-->숫자)\n","# 12000 단어를 기억할 수 있는 단어장 만들기\n","def tokenize(corpus):\n","    # 12000단어를 기억할 수 있는 tokenizer를 만들겁니다\n","    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n","    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","        num_words=12000, \n","        filters=' ',\n","        oov_token=\"<unk>\"\n","    )\n","    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n","    tokenizer.fit_on_texts(corpus)\n","    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n","    tensor = tokenizer.texts_to_sequences(corpus)   \n","    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n","    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n","    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n","    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n","    \n","    print(tensor,tokenizer)\n","    return tensor, tokenizer\n","\n","tensor, tokenizer = tokenize(corpus)"],"execution_count":100,"outputs":[{"output_type":"stream","text":["[[  2 290  28 ...   0   0   0]\n"," [  2 219  13 ...   0   0   0]\n"," [  2  25  15 ...   0   0   0]\n"," ...\n"," [  2  21  77 ...   0   0   0]\n"," [  2  41  26 ...   0   0   0]\n"," [  2  21  77 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f3ff380d150>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DKnpWK1P8Phl"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"dqRVHXOTGrFi"},"source":["### 평가 데이터셋 분리"]},{"cell_type":"code","metadata":{"id":"nY4jgfJ7O_y2","executionInfo":{"status":"ok","timestamp":1626944163350,"user_tz":-540,"elapsed":328,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","# vectorize 된 tensor를 소스와 타겟으로 분리\n","enc_train, enc_val, dec_train, dec_val = train_test_split(tensor[:,:-1], tensor[:, 1:], test_size=0.2)"],"execution_count":101,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJXBCXpkO_wt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626944164674,"user_tz":-540,"elapsed":6,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"9a87c094-a7ba-4ce2-b66e-b08a71ea0abe"},"source":["print(\"Source Train:\", enc_train.shape)\n","print(\"Target Train:\", dec_train.shape)"],"execution_count":102,"outputs":[{"output_type":"stream","text":["Source Train: (124810, 14)\n","Target Train: (124810, 14)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tJYm2GrMOPYP"},"source":["### model 만들기\n","모델의 `Embedding Size`와 `Hidden Size`를 조절하며 10 `Epoch` 안에 `val_loss` 값을 2.2 수준으로 줄일 수 있는 모델을 설계하자"]},{"cell_type":"code","metadata":{"id":"E3_AZVvpO_uf","executionInfo":{"status":"ok","timestamp":1626944169114,"user_tz":-540,"elapsed":320,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}}},"source":["embedding_size = 512 # 워드 벡터의 차원수\n","hidden_size = 2048\n","model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"],"execution_count":103,"outputs":[]},{"cell_type":"code","metadata":{"id":"TI1rpXBCPQzB"},"source":["# for src_sample, tgt_sample in dataset.take(1): break\n","\n","# # 한 배치만 불러온 데이터를 모델에 넣어봅니다\n","# model(src_sample)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kf0hkWDYPEm9"},"source":["# model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A4TN_NCAO_st","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626949287551,"user_tz":-540,"elapsed":5082567,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"33f5ea28-e9fd-4f21-9c0a-0b7f7fddfcb2"},"source":["optimizer = tf.keras.optimizers.Adam()\n","tf.test.is_gpu_available()\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","model.compile(loss=loss, optimizer=optimizer)\n","model.fit(enc_train, dec_train, epochs=10)"],"execution_count":104,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","3901/3901 [==============================] - 523s 130ms/step - loss: 2.9572\n","Epoch 2/10\n","3901/3901 [==============================] - 506s 130ms/step - loss: 2.4203\n","Epoch 3/10\n","3901/3901 [==============================] - 505s 129ms/step - loss: 1.9897\n","Epoch 4/10\n","3901/3901 [==============================] - 506s 130ms/step - loss: 1.6376\n","Epoch 5/10\n","3901/3901 [==============================] - 506s 130ms/step - loss: 1.3869\n","Epoch 6/10\n","3901/3901 [==============================] - 503s 129ms/step - loss: 1.2260\n","Epoch 7/10\n","3901/3901 [==============================] - 507s 130ms/step - loss: 1.1338\n","Epoch 8/10\n","3901/3901 [==============================] - 509s 130ms/step - loss: 1.0831\n","Epoch 9/10\n","3901/3901 [==============================] - 507s 130ms/step - loss: 1.0535\n","Epoch 10/10\n","3901/3901 [==============================] - 507s 130ms/step - loss: 1.0341\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f3ffcc930d0>"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"markdown","metadata":{"id":"91iLbc7BSkRd"},"source":["### 평가하기"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"wl4tmEA3OiEO","executionInfo":{"status":"ok","timestamp":1626949310103,"user_tz":-540,"elapsed":348,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"040565a2-aea2-4c5b-cc4a-e01aeca454f2"},"source":["generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"],"execution_count":105,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> i love you , i m not gonna crack <end> '"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"code","metadata":{"id":"yYqaPGhcO_qh","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1626951190574,"user_tz":-540,"elapsed":340,"user":{"displayName":"김연주","photoUrl":"","userId":"18060504982982500635"}},"outputId":"356ad821-a1ba-427c-8bf0-6189cd6703aa"},"source":["generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=20)"],"execution_count":106,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<start> i m the only one <end> '"]},"metadata":{"tags":[]},"execution_count":106}]},{"cell_type":"code","metadata":{"id":"-KpHSnYdO_oj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V1knM7CSO_nQ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmTzSAEXO_kv"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeXyyXiXNoPC"},"source":[""]},{"cell_type":"code","metadata":{"id":"S_jab9p1O_im"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"507RjNDJO_gi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOQnHT_OO_eh"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"udnJ5zPFO_cb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Djaz8MhbO_aU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcFffXQbO_YI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3KdS-hjcO_Vz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6WJ6VVgO_PA"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z5OVbzjuPAXu"},"source":["todo\n","\n","텍스트 분류 모델을 다루어 보셨다면 Embedding 레이어의 역할에 대해서는 낯설지 않을 것입니다.  "]},{"cell_type":"code","metadata":{"id":"cSgJmK4sPEk-"},"source":[""],"execution_count":null,"outputs":[]}]}